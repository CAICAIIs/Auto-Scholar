# Auto-Scholar Environment Variables
# Copy this file to .env and fill in your values
# NEVER commit .env with real API keys!

# LLM Configuration (Required)
# Get your API key from your LLM provider (OpenAI, DeepSeek, etc.)
# For DeepSeek: LLM_BASE_URL=https://api.deepseek.com/v1, LLM_MODEL=deepseek-chat
# For OpenAI: LLM_BASE_URL=https://api.openai.com/v1, LLM_MODEL=gpt-4o
LLM_API_KEY=your-actual-api-key-here
LLM_BASE_URL=https://api.openai.com/v1
LLM_MODEL=gpt-4o

# AI Runtime Layer Configuration (Optional)
# Path to YAML model configuration file. Takes priority over MODEL_REGISTRY and auto-detection.
# Example: MODEL_CONFIG_PATH=config/models.yaml
# If empty, the system auto-detects models from environment variables.
MODEL_CONFIG_PATH=

# Alternative: JSON string defining available models
# Example: MODEL_REGISTRY=[{"id":"openai:gpt-4o","provider":"openai","model_name":"gpt-4o",...}]
# If empty, auto-detects from LLM_* and provider-specific variables.
MODEL_REGISTRY=

# Default model ID for per-request routing. Format: "provider:model_name"
# Examples: "openai:gpt-4o", "deepseek:deepseek-chat", "ollama:llama3.1:8b"
# If empty, uses LLM_BASE_URL + LLM_MODEL as the default model.
LLM_MODEL_ID=

# Ollama Local Models (Optional)
# Base URL for Ollama's OpenAI-compatible API
OLLAMA_BASE_URL=http://localhost:11434/v1
# Comma-separated list of Ollama models to auto-register
# Example: OLLAMA_MODELS=llama3.1:8b,mistral:7b,gemma:7b
OLLAMA_MODELS=

# Additional Provider Keys (Optional)
# Set these to enable additional providers in the model registry
# DEEPSEEK_API_KEY=your-deepseek-key
# DEEPSEEK_BASE_URL=https://api.deepseek.com/v1

# Performance Tuning (Optional)
# LLM concurrency for parallel operations. Default: 2 (safe for free/low-tier API keys)
# Recommended: 2-4 for free tier, 4-8 for paid tier
LLM_CONCURRENCY=2

# Claim verification concurrency. Default: 2
CLAIM_VERIFICATION_CONCURRENCY=2

# Disable claim verification for time-sensitive scenarios. Default: true
# Set to "false" to reduce workflow time (trade-off: lower citation accuracy)
CLAIM_VERIFICATION_ENABLED=true

# Optional: Semantic Scholar API Key
# Get one at https://www.semanticscholar.org/product/api
# Without this, you'll be rate-limited to ~100 requests/day
SEMANTIC_SCHOLAR_API_KEY=

# Frontend Configuration (for Next.js)
NEXT_PUBLIC_API_URL=http://localhost:8000

# Vector Pipeline Feature Flag (default: false)
# Set to "true" to enable PDF download, vector indexing, and full-text claim verification
# Requires: MinIO, Redis, Qdrant, PostgreSQL services running
VECTOR_PIPELINE_ENABLED=false

# PostgreSQL Configuration
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=autoscholar
POSTGRES_USER=autoscholar
POSTGRES_PASSWORD=autoscholar

# Redis Configuration
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=
REDIS_PDF_CACHE_TTL=86400

# MinIO Configuration
MINIO_ENDPOINT=localhost:9000
MINIO_PORT=9000
MINIO_CONSOLE_PORT=9001
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin
MINIO_SECURE=false
MINIO_BUCKET_RAW=rag-raw
MINIO_BUCKET_PROCESSED=rag-processed
MINIO_BUCKET_TMP=rag-tmp

# Qdrant Configuration
QDRANT_HOST=localhost
QDRANT_PORT=6333
QDRANT_GRPC_PORT=6334
QDRANT_COLLECTION_NAME=paper_chunks

# Embedding Configuration (for vector pipeline)
EMBEDDING_MODEL=text-embedding-3-small
EMBEDDING_DIMENSIONS=1536
EMBEDDING_BATCH_SIZE=100
EMBEDDING_CACHE_TTL=2592000
