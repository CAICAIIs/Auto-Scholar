# Auto-Scholar Environment Variables
# Copy this file to .env and fill in your values
# NEVER commit .env with real API keys!

# LLM Configuration (Required)
# Get your API key from your LLM provider (OpenAI, DeepSeek, etc.)
# For DeepSeek: LLM_BASE_URL=https://api.deepseek.com/v1, LLM_MODEL=deepseek-chat
# For OpenAI: LLM_BASE_URL=https://api.openai.com/v1, LLM_MODEL=gpt-4o
LLM_API_KEY=your-actual-api-key-here
LLM_BASE_URL=https://api.openai.com/v1
LLM_MODEL=gpt-4o

# Multi-Model Support (Optional)
# Default model ID for per-request routing. Format: "provider:model_name"
# Examples: "openai:gpt-4o", "deepseek:deepseek-chat", "ollama:llama3.1:8b"
# If empty, uses LLM_BASE_URL + LLM_MODEL as the default model.
LLM_MODEL_ID=

# Ollama Local Models (Optional)
# Base URL for Ollama's OpenAI-compatible API
OLLAMA_BASE_URL=http://localhost:11434/v1

# Additional Provider Keys (Optional)
# Set these to enable additional providers in the model registry
# DEEPSEEK_API_KEY=your-deepseek-key
# DEEPSEEK_BASE_URL=https://api.deepseek.com/v1

# Optional: Semantic Scholar API Key
# Get one at https://www.semanticscholar.org/product/api
# Without this, you'll be rate-limited to ~100 requests/day
SEMANTIC_SCHOLAR_API_KEY=

# Frontend Configuration (for Next.js)
NEXT_PUBLIC_API_URL=http://localhost:8000
